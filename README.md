-----

# Large-Scale Human Brain Neuron Morphological Reconstruction Pipeline

-----

## ğŸ“– Project Overview

This project presents a comprehensive, end-to-end pipeline for **large-scale morphological reconstruction of human brain neurons from light microscopy imaging data**. Our pipeline addresses the critical challenges in neuron reconstruction, from raw image processing to quantitative analysis, enabling researchers to accurately map intricate neuronal structures at an unprecedented scale and **high throughput**.

Neuron morphological reconstruction is fundamental to understanding brain connectivity, development, and disease. Our pipeline streamlines this complex process, offering robust solutions specifically tailored for **light microscopy data**, making it ideal for large-cohort studies and **high-throughput reconstruction** efforts.

The pipeline comprises several interconnected stages:

1.  **Image Preprocessing**: Enhancing raw imaging data for optimal downstream analysis, with a focus on noise suppression and detail enhancement.
2.  **Image Segmentation**: Identifying individual neurons and their components within the prepared images.
3.  **Tracing**: Reconstructing the intricate 3D paths of neuronal dendrites and axons.
4.  **Quantitative Analysis**: Extracting meaningful morphological features for scientific discovery.

This project is built to be scalable and adaptable, aiming to accelerate neuroscience research by providing an efficient and reliable tool for neuron reconstruction.

-----

## ğŸš€ Features

Here are the key features that make our pipeline stand out:

  * **Designed for ACTomography Data & High Throughput**: Specifically engineered to process **ACTomography light microscopy imaging data**, our pipeline delivers **exceptionally efficient reconstruction** capabilities. This high throughput is critical for tackling the vast datasets generated by modern large-scale brain mapping initiatives.
  * **Intelligent Image Preprocessing**: Our unique preprocessing methods are **biologically informed**, meticulously crafted to both **effectively suppress diffuse noise near the soma (cell body)** and simultaneously **enhance low-contrast details at the distal ends of neuronal branches**. This dual-action ensures clean signals and comprehensive capture of even the finest structures.
  * **Robust Segmentation with Connectivity Supervision**: Built upon the powerful nnU-Net framework, our segmentation process includes **additional supervision specifically focused on the connectivity of the segmentation results**. This ensures more biologically plausible and topologically accurate neuron reconstructions.
  * **Biologically Meaningful Tracing**: Our tracing algorithms are not generic; they are **customized based on the inherent characteristics of neuronal data**, leading to more accurate and biologically relevant 3D reconstructions of complex dendrite and axon morphologies.
  * **Novel Segmentation Loss Function**: We've integrated a **newly designed loss function** within the nnU-Net framework to further optimize neuron segmentation, improving accuracy and robustness for intricate neuronal structures.
  * **End-to-End & Modular Solution**: This is a complete pipeline covering all stages from raw image input to final quantitative morphological metrics. Its modular design allows for independent development, testing, and easy integration of alternative algorithms or future enhancements.
  * **Quantitative Insights**: Provides powerful tools for extracting key morphological features, facilitating in-depth analysis of neuronal architecture essential for scientific discovery.

-----

## ğŸ“¦ Code Structure

The project is organized into a clear, modular structure to facilitate understanding, development, and maintenance.

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original unprocessed image data
â”‚   â””â”€â”€ processed/          # Intermediate processed data from pipeline stages
â”œâ”€â”€ docs/                   # Documentation, diagrams, usage guides
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ nnunet_weights/     # Trained weights for the nnU-Net based segmentation
â”‚   â””â”€â”€ other_models/       # Weights/configs for other ML models (if any)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing/      # Image preprocessing modules
â”‚   â”‚   â”œâ”€â”€ enhance.py      # Implements noise suppression and detail enhancement
â”‚   â”‚   â”œâ”€â”€ register.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â”œâ”€â”€ segmentation/       # Image segmentation modules (nnU-Net based)
â”‚   â”‚   â”œâ”€â”€ nnunet_modifications/ # Specific modifications made to nnU-Net
â”‚   â”‚   â”‚   â”œâ”€â”€ model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ training.py # Modified training script to use custom loss
â”‚   â”‚   â”‚   â””â”€â”€ custom_loss.py # ğŸ‘ˆ Your new loss function definition
â”‚   â”‚   â”œâ”€â”€ segmenter.py    # Wrapper for segmentation inference
â”‚   â”‚   â””â”€â”€ config.py       # Configuration for segmentation
â”‚   â”œâ”€â”€ tracing/            # Neuron tracing algorithms
â”‚   â”‚   â”œâ”€â”€ swc_generator.py # Generates SWC files (standard neuron format)
â”‚   â”‚   â””â”€â”€ algorithm_xyz.py # Example tracing algorithm implementation
â”‚   â”œâ”€â”€ analysis/           # Quantitative analysis modules
â”‚   â”‚   â”œâ”€â”€ metrics.py      # Functions for calculating morphological metrics
â”‚   â”‚   â”œâ”€â”€ visualization.py # Tools for visualizing reconstructed neurons and analysis results
â”‚   â”‚   â””â”€â”€ data_loader.py  # Utilities for loading analysis data
â”‚   â””â”€â”€ pipeline.py         # Main script orchestrating the entire reconstruction pipeline
â”œâ”€â”€ notebooks/              # Jupyter notebooks for experimentation, visualization, or demos
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ run_pipeline.sh     # Example shell script to run the full pipeline
â”‚   â””â”€â”€ train_segmentation.sh # Script to train the segmentation model
â”œâ”€â”€ config/                 # Configuration files for the entire pipeline
â”‚   â”œâ”€â”€ pipeline_config.yaml
â”‚   â””â”€â”€ hardware_config.yaml
â”œâ”€â”€ environment.yaml        # Conda environment definition for dependencies
â”œâ”€â”€ requirements.txt        # Pip requirements for dependencies
â”œâ”€â”€ README.md               # This README file
â””â”€â”€ .gitignore              # Files/directories to ignore in Git
```

### Key Directories and Files:

  * **`data/`**: Stores input and output data. Follows a clear structure for raw and processed stages.
  * **`models/`**: Contains all trained model weights, especially for the segmentation module.
  * **`src/`**: The core source code for the pipeline, organized into logical sub-modules:
      * **`preprocessing/`**: This is where our specialized methods for **simultaneously suppressing diffuse noise around the soma and enhancing faint details at branch terminals** are implemented.
      * **`segmentation/`**: **This is where our modifications to nnU-Net reside.** It includes the `nnunet_modifications` subdirectory, which is crucial. Here you'll find `custom_loss.py` containing your new loss function and a modified `training.py` that integrates it. `segmenter.py` is for inference using the trained models.
      * **`tracing/`**: Contains the algorithms responsible for generating 3D neuron reconstructions, typically outputting `.swc` files.
      * **`analysis/`**: Provides tools for extracting and quantifying morphological features from the reconstructed neurons.
      * **`pipeline.py`**: The central script that orchestrates the execution of all stages in the correct sequence.
  * **`notebooks/`**: Useful for interactive development, testing small components, or demonstrating results.
  * **`scripts/`**: Contains shell scripts for running various parts of the pipeline, useful for automation.
  * **`config/`**: Stores YAML or JSON configuration files for easy pipeline customization without modifying code.
  * **`environment.yaml` / `requirements.txt`**: Defines the project's dependencies for easy environment setup.

-----

## ğŸ› ï¸ Installation

To set up the project environment and run the pipeline, follow these steps:

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/YourUsername/YourRepoName.git
    cd YourRepoName
    ```
2.  **Install nnU-Net:**
    First, install `nnU-Net` by following the official instructions from their GitHub repository: [https://github.com/MIC-DKFZ/nnUNet](https://github.com/MIC-DKFZ/nnUNet). Our segmentation module is built upon and modifies this framework.
3.  **Create and activate the Conda environment** (recommended for project-specific dependencies):
    ```bash
    conda env create -f environment.yaml
    conda activate large-scale-neuron-reconstruction
    ```
    *Alternatively, if you prefer pip for project-specific dependencies:*
    ```bash
    pip install -r requirements.txt
    ```
4.  **Download pre-trained models** (if applicable):
      * Instructions to download trained nnU-Net weights and any other necessary models will go here. E.g., a link to a Google Drive or Hugging Face repository.

-----

## ğŸš€ Usage

Detailed instructions on how to run the pipeline, including examples for each stage and how to customize configurations.

### **Running the Full Pipeline**

```bash
python src/pipeline.py --config config/pipeline_config.yaml
```

### **Running Individual Stages**

  * **Image Preprocessing:**
    ```bash
    python src/preprocessing/enhance.py --input data/raw/image.tiff --output data/processed/enhanced_image.tiff
    ```
  * **Image Segmentation (nnU-Net based):**
      * **Integrating and Training with the New Loss Function:**
        Our custom loss function is defined in `src/segmentation/nnunet_modifications/custom_loss.py`. To train an nnU-Net model using this new loss, our modified training script (`src/segmentation/nnunet_modifications/training.py`) directly incorporates it.
        You'll typically prepare your dataset according to nnU-Net's conventions and then initiate training. For example:
        ```bash
        # This script should wrap the nnU-Net training command
        # and ensure our custom loss is used.
        ./scripts/train_segmentation.sh
        ```
        *Refer to `src/segmentation/nnunet_modifications/training.py` and the main `nnU-Net` documentation for detailed training configurations and dataset setup.*
      * *For running inference:*
        ```bash
        python src/segmentation/segmenter.py --input data/processed/enhanced_image.tiff --output data/processed/segmentation_mask.tiff --model models/nnunet_weights/my_model.pth
        ```
  * **Tracing:**
    ```bash
    python src/tracing/swc_generator.py --segmentation data/processed/segmentation_mask.tiff --output data/processed/reconstruction.swc
    ```
  * **Quantitative Analysis:**
    ```bash
    python src/analysis/metrics.py --swc data/processed/reconstruction.swc --output_csv analysis_results.csv
    ```

-----

## ğŸ¤ Contributing

We welcome contributions to this project\! Please see our [CONTRIBUTING.md](https://www.google.com/search?q=CONTRIBUTING.md) for guidelines on how to submit issues, pull requests, and suggestions.

-----

## ğŸ“œ License

This project is licensed under the [MIT License](https://www.google.com/search?q=LICENSE) - see the [LICENSE](https://www.google.com/search?q=LICENSE) file for details.

-----

## ğŸ“§ Contact

If you have any questions or suggestions, feel free to open an issue in this repository or contact [Your Name/Email] at [your.email@example.com].
